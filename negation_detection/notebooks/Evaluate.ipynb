{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "out_dir = '../data/output/'\n",
    "\n",
    "output_validation_data = pd.read_excel(data_dir + 'DrugVisData - All Annotations.xlsx',\\\n",
    "                                       sheet_name = 'DrugVisData - Copy')\n",
    "\n",
    "output_data_1 = pd.read_csv(out_dir + 'DrugVisData_Negated_Output.csv')\n",
    "output_data_2 = pd.read_excel(out_dir + 'DrugVisData_Negated_Output_2.xlsx', sheet_name='DrugVisData_Negated_Output_2')\n",
    "output_data_3 = pd.read_excel(out_dir + 'DrugVisData_Negated_Output_Parsed.xlsx', sheet_name='DrugVisData_Negated_Output_Pars')\n",
    "output_data_4 = pd.read_csv(out_dir + 'DrugVisdata_Negation_Output_Parsed_D.csv')\n",
    "output_data_5 = pd.read_csv(out_dir + 'DrugVisdata_Negation_Output_Negspacy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_1 = output_data_1.join(output_validation_data[['Unnamed: 0','annotation_expert_1']].set_index('Unnamed: 0'),\\\n",
    "                   on='Unnamed: 0', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])\n",
    "output_data_2 = output_data_2.join(output_validation_data[['Unnamed: 0','annotation_expert_1']].set_index('Unnamed: 0'),\\\n",
    "                   on='Unnamed: 0', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])\n",
    "output_data_3 = output_data_3.join(output_validation_data[['Unnamed: 0','annotation_expert_1']].set_index('Unnamed: 0'),\\\n",
    "                   on='Unnamed: 0', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])\n",
    "output_data_4 = output_data_4.join(output_validation_data[['Unnamed: 0','annotation_expert_1']].set_index('Unnamed: 0'),\\\n",
    "                   on='Unnamed: 0', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])\n",
    "output_data_5 = output_data_5.join(output_validation_data[['Unnamed: 0','annotation_expert_1']].set_index('Unnamed: 0'),\\\n",
    "                   on='Unnamed: 0', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    337\n",
      "1.0     42\n",
      "Name: annotation_expert_1, dtype: int64\n",
      "0.0    337\n",
      "1.0     42\n",
      "Name: annotation_expert_1, dtype: int64\n",
      "0.0    337\n",
      "1.0     42\n",
      "Name: annotation_expert_1, dtype: int64\n",
      "0.0    337\n",
      "1.0     42\n",
      "Name: annotation_expert_1, dtype: int64\n",
      "0.0    337\n",
      "1.0     42\n",
      "Name: annotation_expert_1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(output_data_1['annotation_expert_1'].value_counts())\n",
    "print(output_data_2['annotation_expert_1'].value_counts())\n",
    "print(output_data_3['annotation_expert_1'].value_counts())\n",
    "print(output_data_4['annotation_expert_1'].value_counts())\n",
    "print(output_data_5['annotation_expert_1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    367\n",
       "True      12\n",
       "Name: Is_Negated_Parsed, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data_3['Is_Negated_Parsed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for model using trigger terms V1 (using StanfordCoreNLP to tokenize):\n",
      "\n",
      "[[287  50]\n",
      " [ 30  12]]\n",
      "Confusion Matrix for model using trigger terms V1 (without StanfordCoreNLP to tokenize):\n",
      "\n",
      "[[287  50]\n",
      " [ 30  12]]\n",
      "Confusion Matrix for model using trigger terms and syntactic parsing to identify negated part\n",
      "Checks if negated part contains drug name (done in excel):\n",
      "\n",
      "[[325  12]\n",
      " [ 42   0]]\n",
      "Confusion Matrix for model using scispacy dependency parsing:\n",
      "\n",
      "[[321  16]\n",
      " [ 39   3]]\n",
      "Confusion Matrix for model using scispacy entity recognition and negspacy:\n",
      "\n",
      "[[317  20]\n",
      " [ 34   8]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for model using trigger terms V1 (using StanfordCoreNLP to tokenize):\\n')\n",
    "print(confusion_matrix(output_data_1['annotation_expert_1'], output_data_1['Is_Negated']))\n",
    "\n",
    "print('Confusion Matrix for model using trigger terms V1 (without StanfordCoreNLP to tokenize):\\n')\n",
    "print(confusion_matrix(output_data_2['annotation_expert_1'], output_data_2['Is_Negated']))\n",
    "\n",
    "print('Confusion Matrix for model using trigger terms and syntactic parsing to identify negated part\\nChecks if negated part contains drug name (done in excel):\\n')\n",
    "print(confusion_matrix(output_data_3['annotation_expert_1'], output_data_3['Is_Negated_Parsed']))\n",
    "\n",
    "print('Confusion Matrix for model using scispacy dependency parsing:\\n')\n",
    "print(confusion_matrix(output_data_4['annotation_expert_1'], output_data_4['Is_Negated']))\n",
    "\n",
    "print('Confusion Matrix for model using scispacy entity recognition and negspacy:\\n')\n",
    "print(confusion_matrix(output_data_5['annotation_expert_1'], output_data_5['Is_Negated']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.7889182058047494\n",
      "Precision: 0.1935483870967742\n",
      "Recall: 0.2857142857142857\n",
      "F1 score: 0.23076923076923075\n",
      "\n",
      "\n",
      "Overall accuracy: 0.7889182058047494\n",
      "Precision: 0.1935483870967742\n",
      "Recall: 0.2857142857142857\n",
      "F1 score: 0.23076923076923075\n",
      "\n",
      "\n",
      "Overall accuracy: 0.8575197889182058\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 score: 0.0\n",
      "\n",
      "\n",
      "Overall accuracy: 0.8548812664907651\n",
      "Precision: 0.15789473684210525\n",
      "Recall: 0.07142857142857142\n",
      "F1 score: 0.09836065573770492\n",
      "\n",
      "\n",
      "Overall accuracy: 0.8575197889182058\n",
      "Precision: 0.2857142857142857\n",
      "Recall: 0.19047619047619047\n",
      "F1 score: 0.22857142857142854\n"
     ]
    }
   ],
   "source": [
    "print('Overall accuracy: '\\\n",
    "      + str(accuracy_score(output_data_1['annotation_expert_1'], output_data_1['Is_Negated'] )))\n",
    "print('Precision: '\\\n",
    "      + str(precision_score(output_data_1['annotation_expert_1'], output_data_1['Is_Negated'] )))\n",
    "print('Recall: '\\\n",
    "      + str(recall_score(output_data_1['annotation_expert_1'], output_data_1['Is_Negated'] )))\n",
    "print('F1 score: '\\\n",
    "      + str(f1_score(output_data_1['annotation_expert_1'], output_data_1['Is_Negated'] )))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Overall accuracy: '\\\n",
    "      + str(accuracy_score(output_data_2['annotation_expert_1'], output_data_2['Is_Negated'] )))\n",
    "print('Precision: '\\\n",
    "      + str(precision_score(output_data_2['annotation_expert_1'], output_data_2['Is_Negated'] )))\n",
    "print('Recall: '\\\n",
    "      + str(recall_score(output_data_2['annotation_expert_1'], output_data_2['Is_Negated'] )))\n",
    "print('F1 score: '\\\n",
    "      + str(f1_score(output_data_2['annotation_expert_1'], output_data_2['Is_Negated'] )))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Overall accuracy: '\\\n",
    "      + str(accuracy_score(output_data_3['annotation_expert_1'], output_data_3['Is_Negated_Parsed'] )))\n",
    "print('Precision: '\\\n",
    "      + str(precision_score(output_data_3['annotation_expert_1'], output_data_3['Is_Negated_Parsed'] )))\n",
    "print('Recall: '\\\n",
    "      + str(recall_score(output_data_3['annotation_expert_1'], output_data_3['Is_Negated_Parsed'] )))\n",
    "print('F1 score: '\\\n",
    "      + str(f1_score(output_data_3['annotation_expert_1'], output_data_3['Is_Negated_Parsed'] )))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Overall accuracy: '\\\n",
    "      + str(accuracy_score(output_data_4['annotation_expert_1'], output_data_4['Is_Negated'] )))\n",
    "print('Precision: '\\\n",
    "      + str(precision_score(output_data_4['annotation_expert_1'], output_data_4['Is_Negated'] )))\n",
    "print('Recall: '\\\n",
    "      + str(recall_score(output_data_4['annotation_expert_1'], output_data_4['Is_Negated'] )))\n",
    "print('F1 score: '\\\n",
    "      + str(f1_score(output_data_4['annotation_expert_1'], output_data_4['Is_Negated'] )))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Overall accuracy: '\\\n",
    "      + str(accuracy_score(output_data_5['annotation_expert_1'], output_data_5['Is_Negated'] )))\n",
    "print('Precision: '\\\n",
    "      + str(precision_score(output_data_5['annotation_expert_1'], output_data_5['Is_Negated'] )))\n",
    "print('Recall: '\\\n",
    "      + str(recall_score(output_data_5['annotation_expert_1'], output_data_5['Is_Negated'] )))\n",
    "print('F1 score: '\\\n",
    "      + str(f1_score(output_data_5['annotation_expert_1'], output_data_5['Is_Negated'] )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
