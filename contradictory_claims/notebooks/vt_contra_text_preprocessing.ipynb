{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPreprocessing Cord-19 text files\\n\\nGoal: Filter text and vector files containing publications (1 sentence per row) to publications \\nrelevant to Covid-19 and specified columns.  Merge into a single file for extraction of sentences/vectors\\nbelonging to specified papers/sections.\\n\\nSteps:\\n    1. Filter metadata.csv (from Kaggle Cord-19 dataset) to paper cord_uids for papers where:\\n        -Title or Abstract contains 1 or more Covid-19 synonyms.\\n        -Publication date is after 2019-10-01.\\n        This has been moved to notebook covid_vt_contra_filtering_input_metadata.ipynb\\n        \\n    2.  Load all full text files (parquet format) as pandas dataframes and truncate them to:\\n            -Rows containing cord_uids from step 1\\n            -Columns cord_uid, sentence_id, sentence, and section\\n\\n        Write truncated text dataframes to csv files.\\n\\n    3.  Merge truncated text dataframes and extract list of sentence_ids for truncating vector files.\\n    \\n    4.  Load all vector files (parquet format) as pandas dataframes and truncate them to:\\n            -Rows containing sentence_ids from step 3.\\n        \\n        Write truncated vector dataframes to csv files.\\n    \\n    5.  Merge truncated vector dataframes and join with merged truncated text files (merged_text_vector_df).\\n    \\n    6.  Write merged, truncated vector and text dataframe to csv file.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preprocessing Cord-19 text files\n",
    "\n",
    "Goal: Filter text and vector files containing publications (1 sentence per row) to publications \n",
    "relevant to Covid-19 and specified columns.  Merge into a single file for extraction of sentences/vectors\n",
    "belonging to specified papers/sections.\n",
    "\n",
    "Steps:\n",
    "    1. Filter metadata.csv (from Kaggle Cord-19 dataset) to paper cord_uids for papers where:\n",
    "        -Title or Abstract contains 1 or more Covid-19 synonyms.\n",
    "        -Publication date is after 2019-10-01.\n",
    "        This has been moved to notebook covid_vt_contra_metadata_filtering.ipynb\n",
    "        \n",
    "    2.  Load all full text files (parquet format) as pandas dataframes and truncate them to:\n",
    "            -Rows containing cord_uids from step 1\n",
    "            -Columns cord_uid, sentence_id, sentence, and section\n",
    "\n",
    "        Write truncated text dataframes to csv files.\n",
    "\n",
    "    3.  Merge truncated text dataframes and extract list of sentence_ids for truncating vector files.\n",
    "    \n",
    "    4.  Load all vector files (parquet format) as pandas dataframes and truncate them to:\n",
    "            -Rows containing sentence_ids from step 3.\n",
    "        \n",
    "        Write truncated vector dataframes to csv files.\n",
    "    \n",
    "    5.  Merge truncated vector dataframes and join with merged truncated text files (merged_text_vector_df).\n",
    "    \n",
    "    6.  Write merged, truncated vector and text dataframe to csv file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input and Output file paths.  This will be turned into a config file that can be passed as an argument.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Input\n",
    "resources_path = '../resources/'\n",
    "\n",
    "metadata_csv_path = '%scovid19_date_filt_metadata_200430.csv' % resources_path\n",
    "covid_19_term_list_file = '%scovid_19_terms_200427.txt' % resources_path\n",
    "\n",
    "pq_text_file_dir = '%sv8_preprocessed/' % resources_path\n",
    "pq_vec_file_dir = '%sv8_vectors/' % resources_path\n",
    "\n",
    "pub_date_cutoff = '2019-10-01'\n",
    "\n",
    "#Columns to truncate pq full text files to\n",
    "pq_text_files_cols_oi = ['cord_uid', 'sentence_id', 'section', 'sentence']\n",
    "\n",
    "#Output\n",
    "\n",
    "output_path = '../output/'\n",
    "\n",
    "covid19_filtered_uid_list_file = '%scovid19_cord_uids_200429.txt' % output_path\n",
    "\n",
    "csv_trunc_text_file_dir = '%sv8_truncated_text_files/' % output_path\n",
    "csv_trunc_vec_file_dir = '%sv8_truncated_vec_files/' % output_path\n",
    "\n",
    "full_merged_text_vec_df_outfile_path = '%sfull_merged_text_vector_df_200430.csv' %output_path\n",
    "filt_merged_text_vec_df_outfile_path = '%sfilt_merged_text_vector_df_200430.csv' % output_path\n",
    "\n",
    "cord_uid_text_file_map = '%sv8_id_file_maps/cord_uid_text_file_map.json' % output_path\n",
    "sent_id_text_file_map = '%sv8_id_file_maps/sent_id_text_file_map.json' % output_path\n",
    "sent_id_vec_file_map = '%sv8_id_file_maps/sent_id_vec_file_map.json' % output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef extract_filtered_uid_list_from_metadata_csv(metadata_csv_path, covid_19_term_list_file, pub_date_cutoff):\\n    Filter metadata.csv (from Kaggle Cord-19 dataset) to:\\n        -Papers where 1 or more Covid-19 synonyms is used in title or abstract column.\\n        -Papers published after pub_date_cutoff.\\n    \\n    Return uid_list\\n    metadata_df = pd.read_csv(metadata_csv_path)\\n    metadata_df = metadata_df.fillna(\\'\\')\\n    metadata_df.loc[:, \\'title_abstract\\'] = metadata_df.loc[:, \\'title\\'].str.lower() + \\' \\' + metadata_df.loc[:, \\'abstract\\'].str.lower()\\n    metadata_df.loc[:, \\'title_abstract\\'] = metadata_df.loc[:, \\'title_abstract\\'].fillna(\\'\\')\\n\\n    with open(covid_19_term_list_file) as f:\\n        covid_19_terms = f.read().splitlines()\\n        covid_19_term_pattern = \\'|\\'.join([i.lower() for i in covid_19_terms])\\n\\n    covid19_df = metadata_df.loc[metadata_df.title_abstract.str.contains(covid_19_term_pattern)]\\n\\n    covid19_uid_list = covid19_df.cord_uid.tolist()\\n    print(\"Covid-19 uids filtered by use of Covid-19 synonyms: %d\" % len(covid19_uid_list))\\n    \\n    covid19_date_filtered_df = covid19_df.loc[covid19_df[\\'publish_time\\'] > pub_date_cutoff]\\n    print(\"Covid-19 df filtered to publication dates after %s: %d\" % (pub_date_cutoff, len(covid19_date_filtered_df)))\\n\\n    covid19_date_incl_cord_uids = set(covid19_date_filtered_df.cord_uid.tolist())\\n    date_excl_uids = set(covid19_uid_list) - set(covid19_date_incl_cord_uids)\\n\\n    print(\"Papers included by date cutoff filter: %d\" % len(covid19_date_incl_cord_uids))\\n    print(\"Papers excluded by date cutoff filter: %d\" % len(date_excl_uids))\\n    \\n    return covid19_date_incl_cord_uids'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1\n",
    "#Separated this into notebook covid_vt_contra_metadata_filtering.ipynb\n",
    "\"\"\"\n",
    "def extract_filtered_uid_list_from_metadata_csv(metadata_csv_path, covid_19_term_list_file, pub_date_cutoff):\n",
    "    Filter metadata.csv (from Kaggle Cord-19 dataset) to:\n",
    "        -Papers where 1 or more Covid-19 synonyms is used in title or abstract column.\n",
    "        -Papers published after pub_date_cutoff.\n",
    "    \n",
    "    Return uid_list\n",
    "    metadata_df = pd.read_csv(metadata_csv_path)\n",
    "    metadata_df = metadata_df.fillna('')\n",
    "    metadata_df.loc[:, 'title_abstract'] = metadata_df.loc[:, 'title'].str.lower() + ' ' + metadata_df.loc[:, 'abstract'].str.lower()\n",
    "    metadata_df.loc[:, 'title_abstract'] = metadata_df.loc[:, 'title_abstract'].fillna('')\n",
    "\n",
    "    with open(covid_19_term_list_file) as f:\n",
    "        covid_19_terms = f.read().splitlines()\n",
    "        covid_19_term_pattern = '|'.join([i.lower() for i in covid_19_terms])\n",
    "\n",
    "    covid19_df = metadata_df.loc[metadata_df.title_abstract.str.contains(covid_19_term_pattern)]\n",
    "\n",
    "    covid19_uid_list = covid19_df.cord_uid.tolist()\n",
    "    print(\"Covid-19 uids filtered by use of Covid-19 synonyms: %d\" % len(covid19_uid_list))\n",
    "    \n",
    "    covid19_date_filtered_df = covid19_df.loc[covid19_df['publish_time'] > pub_date_cutoff]\n",
    "    print(\"Covid-19 df filtered to publication dates after %s: %d\" % (pub_date_cutoff, len(covid19_date_filtered_df)))\n",
    "\n",
    "    covid19_date_incl_cord_uids = set(covid19_date_filtered_df.cord_uid.tolist())\n",
    "    date_excl_uids = set(covid19_uid_list) - set(covid19_date_incl_cord_uids)\n",
    "\n",
    "    print(\"Papers included by date cutoff filter: %d\" % len(covid19_date_incl_cord_uids))\n",
    "    print(\"Papers excluded by date cutoff filter: %d\" % len(date_excl_uids))\n",
    "    \n",
    "    return covid19_date_incl_cord_uids\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2\n",
    "\n",
    "def truncate_pq_text_file(pq_text_file, pq_text_files_cols_oi, uid_list):\n",
    "    \"\"\"\n",
    "    Load text file as dataframe, filter to relevant columns (cord_uid, sentence_id, section, sentence) and \n",
    "    cord_uids for Covid-19 papers.    \n",
    "    \n",
    "    Return truncated dataframe.\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Loading dataframe from %s\" % pq_text_file)\n",
    "    text_file_df = pd.read_parquet(pq_text_file)\n",
    "    text_file_trunc_df = text_file_df[pq_text_files_cols_oi]\n",
    "    text_file_trunc_df = text_file_trunc_df.loc[text_file_trunc_df.cord_uid.isin(uid_list)]\n",
    "    \n",
    "    return text_file_trunc_df\n",
    "    \n",
    "def truncate_pq_text_files(pq_text_file_dir, pq_text_files_cols_oi, csv_trunc_text_file_dir, uid_list):\n",
    "    \"\"\"\n",
    "    For each full text parquet file in pq_text_file_dir, load as dataframe, \n",
    "    filter to specified cord_uids and columns of interest, and write to csv file in csv_trunc_text_file_dir\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Parquet text files input directory: %s\" % pq_text_file_dir)\n",
    "    pq_text_files = glob.glob('%s*' % pq_text_file_dir)\n",
    "    print(\"Files in text files input directory: %d\" % len(pq_text_files))\n",
    "\n",
    "    print(\"Parquet text files output directory: %s\" % csv_trunc_text_file_dir)\n",
    "    print('\\n')\n",
    "\n",
    "    for pq_text_file in pq_text_files:\n",
    "        \n",
    "        text_file_trunc_df = truncate_pq_text_file(pq_text_file, pq_text_files_cols_oi, uid_list)\n",
    "        text_file_name = pq_text_file.split('.')[0].split('/')[-1]\n",
    "        outfile = '%s%s_trunc.csv' %(csv_trunc_text_file_dir, text_file_name)\n",
    "    \n",
    "        print(\"Writing truncated text dataframe to %s\" % outfile)\n",
    "        text_file_trunc_df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3, 5\n",
    "\n",
    "def concat_dataframes_from_dir_csvs(trunc_csv_dir):\n",
    "\n",
    "    trunc_csv_files = glob.glob('%s*' % trunc_csv_dir)\n",
    "    print(\"Files in truncated csv output directory: %d\" % len(trunc_csv_files))\n",
    "\n",
    "    trunc_dfs = []\n",
    "    for trunc_csv_file in trunc_csv_files:\n",
    "        trunc_dfs.append(pd.read_csv(trunc_csv_file, index_col=0))\n",
    "    \n",
    "    merged_trunc_df = pd.concat(trunc_dfs)\n",
    "    return merged_trunc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4\n",
    "\n",
    "def truncate_pq_vec_file(pq_vec_file, sentence_ids):\n",
    "    vec_file_df = pd.read_parquet(pq_vec_file)\n",
    "    vec_file_trunc_df = vec_file_df.loc[vec_file_df.sentence_id.isin(sentence_ids)]\n",
    "    \n",
    "    return vec_file_trunc_df\n",
    "\n",
    "def truncate_pq_vec_files(pq_vec_file_dir, sentence_ids, csv_trunc_vec_file_dir):\n",
    "    pq_vec_files = glob.glob('%s*' % pq_vec_file_dir)\n",
    "    print(\"Files in vector file input directory: %d\" % len(pq_vec_files))\n",
    "\n",
    "    for pq_vec_file in pq_vec_files:\n",
    "        vec_file_trunc_df = truncate_pq_vec_file(pq_vec_file, sentence_ids)\n",
    "        \n",
    "        vec_file_name = pq_vec_file.split('.')[0].split('/')[-1]\n",
    "        outfile = '%s%s_trunc.csv' % (csv_trunc_vec_file_dir, vec_file_name)\n",
    "\n",
    "        print(\"Writing truncated vector dataframe to %s\" % outfile)\n",
    "        vec_file_trunc_df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Filtering metadata csv by Covid-19 synoynms and publication date...\n",
      "Step 2: Loading full text dataframes, filtering to extracted uids, and writing to csv files...\n",
      "Parquet text files input directory: resources/v8_preprocessed/\n",
      "Files in text files input directory: 20\n",
      "Parquet text files output directory: resources/v8_truncated_text_files/\n",
      "\n",
      "\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText1.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText1_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText4.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText4_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText8.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText8_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText10.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText10_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText3.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText3_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText0.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText0_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText12.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText12_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText15.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText15_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText2.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText2_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText6.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText6_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText19.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText19_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText18.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText18_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText11.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText11_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText13.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText13_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText5.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText5_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText7.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText7_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText17.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText17_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText16.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText16_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText14.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText14_trunc.csv\n",
      "Loading dataframe from resources/v8_preprocessed/v8processedText9.parquet\n",
      "Writing truncated text dataframe to resources/v8_truncated_text_files/v8processedText9_trunc.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "\n",
    "print(\"Step 1: Filtering metadata csv by Covid-19 synoynms and publication date...\")\n",
    "#uid_list = extract_filtered_uid_list_from_metadata_csv(metadata_csv_path, covid_19_term_list_file, pub_date_cutoff)\n",
    "#print('\\n')\n",
    "\n",
    "#Replacing this step with the output of notebook covid_vt_contra_metadata_filtering.ipynb\n",
    "covid19_df = pd.read_csv(metadata_csv_path)\n",
    "uid_list = covid19_df.cord_uid.tolist()\n",
    "\n",
    "print(\"Step 2: Loading full text dataframes, filtering to extracted uids, and writing to csv files...\")\n",
    "truncate_pq_text_files(pq_text_file_dir, pq_text_files_cols_oi, csv_trunc_text_file_dir, uid_list)    \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Merging truncated text dataframes and extracting sentence ids...\n",
      "Files in truncated csv output directory: 20\n",
      "Number of sentence ids: 294597\n",
      "Number of unique sentence ids: 294597\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Merging truncated text dataframes and extracting sentence ids...\")\n",
    "concat_text_trunc_df = concat_dataframes_from_dir_csvs(csv_trunc_text_file_dir)\n",
    "text_sentence_ids = concat_text_trunc_df.sentence_id.tolist()\n",
    "print(\"Number of sentence ids: %d\" % len(text_sentence_ids))\n",
    "print(\"Number of unique sentence ids: %d\" % len(set(text_sentence_ids)))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Loading vector dataframes, filtering to extracted sentence ids, and writing to csv files...\n",
      "Files in vector file input directory: 20\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs16_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs19_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs1_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs18_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs17_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs4_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs5_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs2_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs14_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs11_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs9_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs10_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs7_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs3_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs12_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs13_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs15_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs0_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs8_trunc.csv\n",
      "Writing truncated vector dataframe to resources/v8_truncated_vec_files/v8processedVecs6_trunc.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Loading vector dataframes, filtering to extracted sentence ids, and writing to csv files...\")\n",
    "truncate_pq_vec_files(pq_vec_file_dir, text_sentence_ids, csv_trunc_vec_file_dir)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Merging truncated vector dataframes...\n",
      "Files in truncated csv output directory: 20\n",
      "\n",
      "\n",
      "Step 6: Merging concatenated text dataframe and concatenated vector dataframe on sentence id...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Merging truncated vector dataframes...\")\n",
    "concat_vec_trunc_df = concat_dataframes_from_dir_csvs(csv_trunc_vec_file_dir)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Step 6: Merging concatenated text dataframe and concatenated vector dataframe on sentence id...\")\n",
    "merged_text_vec_df = pd.DataFrame.merge(concat_text_trunc_df, concat_vec_trunc_df, on='sentence_id')\n",
    "merged_text_vec_df.to_csv(full_merged_text_vec_df_outfile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions to generate regex match patterns from synonymous words/phrases for filtering subject headers\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_regex_pattern(section_list, pattern):\n",
    "    r = re.compile(pattern, re.IGNORECASE)\n",
    "    extracted_list = list(filter(r.match, section_list))\n",
    "    remaining_list = list(set(section_list) - set(extracted_list))\n",
    "    \n",
    "    return remaining_list, extracted_list\n",
    "\n",
    "def construct_regex_match_pattern(terms_dict):\n",
    "    fuzzy_terms = ['.*%s.*' % i for i in terms_dict['fuzzy']]\n",
    "    exact_terms = terms_dict['exact']\n",
    "    \n",
    "    fuzzy_pattern = '|'.join(fuzzy_terms)\n",
    "    #exact_pattern = '|'.join(exact_terms)\n",
    "    \n",
    "    full_pattern = fuzzy_pattern\n",
    "    \n",
    "    return full_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract putative discussion headers.\n",
    "\"\"\"\n",
    "\n",
    "#Construct regex pattern for discussion header terms and extract list of matching headers.\n",
    "\n",
    "disc_terms_dict = {\n",
    "    'exact': [],\n",
    "    'fuzzy' : [\n",
    "        'conclusion',\n",
    "        'discussion',\n",
    "        'interpretation',\n",
    "        'added value of this study',\n",
    "        'research in context',\n",
    "        'concluding',\n",
    "        'closing remarks',\n",
    "        'summary of findings',\n",
    "        'outcome'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*conclusion.*|.*discussion.*|.*interpretation.*|.*added value of this study.*|.*research in context.*|.*concluding.*|.*closing remarks.*|.*summary of findings.*|.*outcome.*\n"
     ]
    }
   ],
   "source": [
    "conc_pattern = construct_regex_match_pattern(disc_terms_dict)\n",
    "print(conc_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discussion headers: 375\n",
      "Example discussion headers:\n",
      "['Study Outcomes ::: Methods',\n",
      " 'Conclusion & Future Directions 132',\n",
      " 'Milling and Optical Transparency ::: 3.2. Fabrication ::: 3. Results and '\n",
      " 'Discussion',\n",
      " 'Final Concluding Remarks based on our COVID-19 Predictions in China',\n",
      " 'IMAGINE INTERPRETATION',\n",
      " 'V. DISCUSSION',\n",
      " 'Discussions',\n",
      " 'Interpretation:',\n",
      " 'Summary of policy measures on key outcome measures',\n",
      " '4. Conclusions',\n",
      " 'CONCLUSIONS',\n",
      " '3.3. Metal Adhesion ::: 3. Results and Discussion',\n",
      " 'What is the causative agent of disease? ::: DISCUSSION',\n",
      " 'Discussion 173',\n",
      " 'Efficacy until Day 6 outcomes',\n",
      " 'Overall clinical features and outcome',\n",
      " 'Outcome of patients treated with CP as compared to a recent historic control '\n",
      " 'groupAhistoric',\n",
      " 'Outcome definitions',\n",
      " 'Graph interpretation',\n",
      " 'Outcomes pool']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "unique_sections = set(merged_text_vec_df.section.tolist())\n",
    "\n",
    "rem_header_list, ext_header_list = extract_regex_pattern(unique_sections, conc_pattern)\n",
    "\n",
    "print(\"Number of discussion headers: %d\" % len(ext_header_list))\n",
    "print(\"Example discussion headers:\")\n",
    "pprint.pprint(ext_header_list[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sections filtering to: 336\n"
     ]
    }
   ],
   "source": [
    "#For now, lower-casing all section headers for matching\n",
    "\n",
    "section_exact_match_list = set([i.lower() for i in ext_header_list])\n",
    "section_exact_match_list.add('abstract')\n",
    "section_exact_match_list.add('title')\n",
    "\n",
    "print(\"Number of sections filtering to: %d\" % len(section_exact_match_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text_vec_df = pd.read_csv(full_merged_text_vec_df_outfile_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered_to_sections_oi = merged_text_vec_df.loc[merged_text_vec_df.section.str.lower().isin(section_exact_match_list)]\n",
    "\n",
    "merged_df_filtered_to_sections_oi.to_csv(filt_merged_text_vec_df_outfile_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filtering out sentences with less than three non-header words.\n",
    "\n",
    "Author: Malavika Suresh\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_data_path = 'resources/'\n",
    "input_data_file = 'filt_merged_text_vector_df_200430.csv'\n",
    "\n",
    "input_data = pd.read_csv(input_data_path + input_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only sentences containing at least 3 words other than those defined below\n",
    "#This also removes any sentences that do not contain any words at all\n",
    "\n",
    "rep = {\"text\": \"\", \"cite_spans\": \"\", \"ref_spans\": \"\", \"section\": \"\", \"Abstract\": \"\",\\\n",
    "       \"bioRxiv preprint\": \"\", \"medRxiv preprint\": \"\", \"doi:\": \"\"}\n",
    "rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "sentences_temp = [pattern.sub(lambda m: rep[re.escape(m.group(0))], s) for s in input_data.sentence]\n",
    "pattern = re.compile(\".*[A-Za-z].*\")\n",
    "sentences_to_keep = [(bool(re.search(pattern,s))) & (len(s.split(' '))>2) for s in sentences_temp]\n",
    "input_processed = input_data.loc[sentences_to_keep,:]\n",
    "sentences_to_drop = [not i for i in sentences_to_keep]\n",
    "input_excluded = input_data.loc[sentences_to_drop,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slander/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "#Convert w2vVector column from string type to  list\n",
    "input_processed.w2vVector = [re.sub(',+', ',', ','.join(w.replace('\\n','').split(' '))) for w in input_processed.w2vVector]\n",
    "input_processed.w2vVector = [re.sub('\\[,', '', w) for w in input_processed.w2vVector]\n",
    "input_processed.w2vVector = [re.sub(',\\]', '', w) for w in input_processed.w2vVector]\n",
    "input_processed.w2vVector = [re.sub('\\[', '', w) for w in input_processed.w2vVector]\n",
    "input_processed.w2vVector = [re.sub('\\]', '', w) for w in input_processed.w2vVector]\n",
    "input_processed.w2vVector = input_processed.w2vVector.apply(lambda s: list(ast.literal_eval(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_processed.to_csv('%scord_titles_abstracts_conclusions.csv' % output_path)\n",
    "input_excluded.to_csv('%scord_titles_abstracts_conclusions_excluded.csv' % output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "      <th>w2vVector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cc29v5qw</td>\n",
       "      <td>cc29v5qw029827</td>\n",
       "      <td>title</td>\n",
       "      <td>Patients with mental health disorders in the C...</td>\n",
       "      <td>[-0.14722456, 0.01686829, 0.16839595, -0.04342...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oribgtyl</td>\n",
       "      <td>oribgtyl031852</td>\n",
       "      <td>title</td>\n",
       "      <td>Prisons and custodial settings are part of a c...</td>\n",
       "      <td>[-0.0138100795, 0.00680788979, 0.102320798, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6r27qzap</td>\n",
       "      <td>6r27qzap031865</td>\n",
       "      <td>title</td>\n",
       "      <td>The resilience of the Spanish health system ag...</td>\n",
       "      <td>[0.00911541, -0.02315879, 0.11453069, -0.05539...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65b267ic</td>\n",
       "      <td>65b267ic032159</td>\n",
       "      <td>title</td>\n",
       "      <td>Six weeks into the 2019 coronavirus disease (C...</td>\n",
       "      <td>[-0.02097336, 0.02406202, 0.06325101, -0.14176...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cja8i0hw</td>\n",
       "      <td>cja8i0hw032165</td>\n",
       "      <td>title</td>\n",
       "      <td>The Novel Coronavirus: A Bird's Eye View</td>\n",
       "      <td>[0.107559, -0.14134592, -0.0632042, -0.0697434...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73032</th>\n",
       "      <td>647zcjgu</td>\n",
       "      <td>647zcjgu1219583</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Among them are captopril, perindopril, ramipri...</td>\n",
       "      <td>[0.11334185, 0.11791535, 0.11523059, 0.0925779...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73033</th>\n",
       "      <td>647zcjgu</td>\n",
       "      <td>647zcjgu1219584</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Although these drugs primarily target ACE, a h...</td>\n",
       "      <td>[0.05551118, -0.01351563, 0.18281737, -0.04538...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73034</th>\n",
       "      <td>647zcjgu</td>\n",
       "      <td>647zcjgu1219585</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>It should be noted that ACE inhibitors bind to...</td>\n",
       "      <td>[0.04974999, -0.06304489, 0.10167421, 0.020741...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73035</th>\n",
       "      <td>647zcjgu</td>\n",
       "      <td>647zcjgu1219586</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Nonetheless, these enzymatic inhibitors may in...</td>\n",
       "      <td>[0.00876493, -0.06595846, 0.09507835, -0.01413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73036</th>\n",
       "      <td>647zcjgu</td>\n",
       "      <td>647zcjgu1219587</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>It is certainly worthwhile to test these drugs...</td>\n",
       "      <td>[-0.0786667541, 0.00177989108, 0.0596073009, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69503 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cord_uid      sentence_id     section  \\\n",
       "0      cc29v5qw   cc29v5qw029827       title   \n",
       "2      oribgtyl   oribgtyl031852       title   \n",
       "4      6r27qzap   6r27qzap031865       title   \n",
       "6      65b267ic   65b267ic032159       title   \n",
       "8      cja8i0hw   cja8i0hw032165       title   \n",
       "...         ...              ...         ...   \n",
       "73032  647zcjgu  647zcjgu1219583  Discussion   \n",
       "73033  647zcjgu  647zcjgu1219584  Discussion   \n",
       "73034  647zcjgu  647zcjgu1219585  Discussion   \n",
       "73035  647zcjgu  647zcjgu1219586  Discussion   \n",
       "73036  647zcjgu  647zcjgu1219587  Discussion   \n",
       "\n",
       "                                                sentence  \\\n",
       "0      Patients with mental health disorders in the C...   \n",
       "2      Prisons and custodial settings are part of a c...   \n",
       "4      The resilience of the Spanish health system ag...   \n",
       "6      Six weeks into the 2019 coronavirus disease (C...   \n",
       "8               The Novel Coronavirus: A Bird's Eye View   \n",
       "...                                                  ...   \n",
       "73032  Among them are captopril, perindopril, ramipri...   \n",
       "73033  Although these drugs primarily target ACE, a h...   \n",
       "73034  It should be noted that ACE inhibitors bind to...   \n",
       "73035  Nonetheless, these enzymatic inhibitors may in...   \n",
       "73036  It is certainly worthwhile to test these drugs...   \n",
       "\n",
       "                                               w2vVector  \n",
       "0      [-0.14722456, 0.01686829, 0.16839595, -0.04342...  \n",
       "2      [-0.0138100795, 0.00680788979, 0.102320798, 0....  \n",
       "4      [0.00911541, -0.02315879, 0.11453069, -0.05539...  \n",
       "6      [-0.02097336, 0.02406202, 0.06325101, -0.14176...  \n",
       "8      [0.107559, -0.14134592, -0.0632042, -0.0697434...  \n",
       "...                                                  ...  \n",
       "73032  [0.11334185, 0.11791535, 0.11523059, 0.0925779...  \n",
       "73033  [0.05551118, -0.01351563, 0.18281737, -0.04538...  \n",
       "73034  [0.04974999, -0.06304489, 0.10167421, 0.020741...  \n",
       "73035  [0.00876493, -0.06595846, 0.09507835, -0.01413...  \n",
       "73036  [-0.0786667541, 0.00177989108, 0.0596073009, -...  \n",
       "\n",
       "[69503 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_data = input_processed.loc[input_processed.section=='title',:]\n",
    "abstract_data = input_processed.loc[input_processed.section=='abstract',:]\n",
    "conclusion_data = input_processed.loc[(input_processed.section!='title') & (input_processed.section!='abstract'),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 4677\n",
      "Number of papers with title: 4663\n",
      "Number of papers with abstract: 2992\n",
      "Number of papers with conclusion: 1662\n"
     ]
    }
   ],
   "source": [
    "print('Number of papers:', input_data.cord_uid.nunique())\n",
    "print('Number of papers with title:', title_data.cord_uid.nunique())\n",
    "print('Number of papers with abstract:', abstract_data.cord_uid.nunique())\n",
    "print('Number of papers with conclusion:', conclusion_data.cord_uid.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique sentences under titles: 4819\n",
      "Number of unique sentence ids under titles: 4903\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique sentences under titles:', title_data.sentence.nunique())\n",
    "print('Number of unique sentence ids under titles:', title_data.sentence_id.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average w2v vectors of all sentences falling under a single cord_uid\n",
    "title_data_final = pd.DataFrame(columns = ['cord_uid','sentence','w2vVector'])\n",
    "for cord_uid in title_data.cord_uid.unique():\n",
    "    title = \" \".join(title_data.loc[title_data.cord_uid==cord_uid,'sentence'])\n",
    "    w2vVector = np.mean(list(title_data.loc[title_data.cord_uid==cord_uid,'w2vVector']), axis=0)\n",
    "    title_data_final = title_data_final.append({'cord_uid':cord_uid,\\\n",
    "                                                'sentence': title,\\\n",
    "                                                'w2vVector': w2vVector},\\\n",
    "                                               ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_similarity.to_csv('%stitle_similarity_sample' % output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6714913949640489"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(title_data_final.w2vVector[0].reshape(1,-1),title_data_final.w2vVector[1].reshape(1,-1))[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_similarity = pd.DataFrame(columns=['paper1_cord_uid','paper2_cord_uid','title1','title2','similarity_score'])\n",
    "jit(nopython=True, parallel=True)\n",
    "for i,paper1 in enumerate(title_data_final.sentence[:50]):\n",
    "    for j,paper2 in enumerate(title_data_final.sentence[:50]):\n",
    "        if i!=j:\n",
    "            cos_sim = cosine_similarity(title_data_final.w2vVector[i].reshape(1,-1),title_data_final.w2vVector[j].reshape(1,-1))[0][0]\n",
    "            title_similarity = title_similarity.append({'paper1_cord_uid':title_data_final.cord_uid[i],\\\n",
    "                                                        'paper2_cord_uid':title_data_final.cord_uid[j],\\\n",
    "                                                        'title1':paper1,\\\n",
    "                                                        'title2':paper2,\\\n",
    "                                                        'similarity_score':cos_sim},\\\n",
    "                                               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_similarity.to_csv('%stitle_similarity_sample' % output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "QC:\n",
    "    -sentence_id and w2vVector pairings match original vector files\n",
    "    -sentence_id and sentence pairings match original text files\n",
    "    -sentence_id and cord_uid pairings matches original text files\n",
    "\n",
    "    -sentences in df are in appropriate order\n",
    "    -filtered sections are logical\n",
    "\n",
    "    -recalculate number of cord_uids with title, abstract, and discussion sections\n",
    "\"\"\"\n",
    "\n",
    "import random \n",
    "\n",
    "#Random sample of sentence_ids\n",
    "sample_sent_ids = random.sample(merged_df_filtered_to_sections_oi.sentence_id.tolist(), 20)\n",
    "sample_cord_uids = random.sample(merged_df_filtered_to_sections_oi.cord_uid.tolist(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_preprocessed/v8processedText1.parquet\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Map cord_uids to text files and write to json.\n",
    "\"\"\"\n",
    "\n",
    "pq_text_files = glob.glob('%s/*' % pq_text_file_dir)\n",
    "\n",
    "text_file_cord_uids_dict = {}\n",
    "cord_uids_text_file_dict = {}\n",
    "\n",
    "sent_id_sentence_dict = {}\n",
    "sentence_sent_id_dict = {}\n",
    "\n",
    "for pq_text_file in pq_text_files:\n",
    "    print(pq_text_file)\n",
    "    pq_text_df = pd.read_parquet(pq_text_file)\n",
    "    \n",
    "    text_file_uids = list(set(pq_text_df.cord_uid.tolist()))\n",
    "    \n",
    "    for text_file_uid in text_file_uids:\n",
    "        text_file_cord_uids_dict.setdefault(pq_text_file, set([])).add(text_file_uid)\n",
    "        cord_uids_text_file_dict.setdefault(text_file_uid, set([])).add(pq_text_file)\n",
    "        \n",
    "with open(cord_uid_text_file_map, 'w') as f:\n",
    "    json.dump(cord_uids_text_file_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_preprocessed/v8processedText16.parquet\n",
      "      cord_uid     sentence_id  \\\n",
      "1146  g8saag2o   g8saag2o01146   \n",
      "1147  g8saag2o   g8saag2o01147   \n",
      "1148  g8saag2o   g8saag2o01148   \n",
      "1149  g8saag2o   g8saag2o01149   \n",
      "1150  g8saag2o   g8saag2o01150   \n",
      "...        ...             ...   \n",
      "1279  g8saag2o   g8saag2o71279   \n",
      "1280  g8saag2o  g8saag2o111280   \n",
      "1281  g8saag2o  g8saag2o111281   \n",
      "1282  g8saag2o  g8saag2o121282   \n",
      "1283  g8saag2o  g8saag2o121283   \n",
      "\n",
      "                                               sentence  \n",
      "1146  Care of patients with liver disease during the...  \n",
      "1147  [{'text': 'The coronavirus disease 2019 pandem...  \n",
      "1148  Older patients and those with pre-existing med...  \n",
      "1149  It remains unclear at this point to what exten...  \n",
      "1150  However, patients with advanced liver disease ...  \n",
      "...                                                 ...  \n",
      "1279  Routine laboratory testing can be performed lo...  \n",
      "1280  General considerations Care should be maintain...  \n",
      "1281  Listing for transplantation should be restrict...  \n",
      "1282  Reducing the in-hospital liver transplant eval...  \n",
      "1283  Emphasis on the importance of vaccination for ...  \n",
      "\n",
      "[138 rows x 3 columns]\n",
      "resources/v8_preprocessed/v8processedText10.parquet\n",
      "       cord_uid     sentence_id  \\\n",
      "5691   f1gum03h   f1gum03h05691   \n",
      "5692   f1gum03h   f1gum03h05692   \n",
      "5693   f1gum03h   f1gum03h05693   \n",
      "5694   f1gum03h   f1gum03h05694   \n",
      "5695   f1gum03h   f1gum03h05695   \n",
      "...         ...             ...   \n",
      "11627  9igk3ke1  9igk3ke1811627   \n",
      "11628  9igk3ke1  9igk3ke1811628   \n",
      "11629  9igk3ke1  9igk3ke1811629   \n",
      "11630  9igk3ke1  9igk3ke1811630   \n",
      "11631  9igk3ke1  9igk3ke1811631   \n",
      "\n",
      "                                                sentence  \n",
      "5691   Crisis Symptom Management and Patient Communic...  \n",
      "5692   [{'text': 'Symptom management and skilled comm...  \n",
      "5693   While palliative care specialists have trainin...  \n",
      "5694   It is imperative that all clinicians respondin...  \n",
      "5695   ', 'cite_spans': [], 'ref_spans': [], 'section...  \n",
      "...                                                  ...  \n",
      "11627  Zinc is the main structural component of aroun...  \n",
      "11628  The deficiency of zinc also modifies the devel...  \n",
      "11629  The function of macrophage also is adversely a...  \n",
      "11630  Zinc deficiency is surprisingly common in mode...  \n",
      "11631  Zinc deficiency impairs the antiviral immunity...  \n",
      "\n",
      "[312 rows x 3 columns]\n",
      "resources/v8_preprocessed/v8processedText11.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d9c2f4ff050a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtext_file_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msubset_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_file_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_file_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcord_uid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_cord_uids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_pandas_metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         result = self.api.parquet.read_table(\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         ).to_pandas()\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[0;34m(options, table, categories, ignore_metadata)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0m_check_data_column_metadata_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_table_to_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_column_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m_table_to_blocks\u001b[0;34m(options, block_table, categories)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;31m# Convert an arrow table to Block from the internal pandas API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_to_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;31m# Defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_text_files = set([])\n",
    "\n",
    "for cord_uid in sample_cord_uids:\n",
    "    sample_text_files.add(list(cord_uids_text_file_dict[cord_uid])[0])\n",
    "\n",
    "for text_file in list(sample_text_files):\n",
    "    print(text_file)\n",
    "    text_file_df = pd.read_parquet(text_file)\n",
    "    \n",
    "    subset_df = text_file_df.loc[text_file_df.cord_uid.isin(sample_cord_uids)]\n",
    "    print(subset_df[['cord_uid', 'sentence_id', 'sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_vectors/v8processedVecs16.parquet\n",
      "resources/v8_vectors/v8processedVecs19.parquet\n",
      "resources/v8_vectors/v8processedVecs1.parquet\n",
      "resources/v8_vectors/v8processedVecs18.parquet\n",
      "resources/v8_vectors/v8processedVecs17.parquet\n",
      "resources/v8_vectors/v8processedVecs4.parquet\n",
      "resources/v8_vectors/v8processedVecs5.parquet\n",
      "resources/v8_vectors/v8processedVecs2.parquet\n",
      "resources/v8_vectors/v8processedVecs14.parquet\n",
      "resources/v8_vectors/v8processedVecs11.parquet\n",
      "resources/v8_vectors/v8processedVecs9.parquet\n",
      "resources/v8_vectors/v8processedVecs10.parquet\n",
      "resources/v8_vectors/v8processedVecs7.parquet\n",
      "resources/v8_vectors/v8processedVecs3.parquet\n",
      "resources/v8_vectors/v8processedVecs12.parquet\n",
      "resources/v8_vectors/v8processedVecs13.parquet\n",
      "resources/v8_vectors/v8processedVecs15.parquet\n",
      "resources/v8_vectors/v8processedVecs0.parquet\n",
      "resources/v8_vectors/v8processedVecs8.parquet\n",
      "resources/v8_vectors/v8processedVecs6.parquet\n"
     ]
    }
   ],
   "source": [
    "pq_vec_files = glob.glob('%s/*' % pq_vec_file_dir)\n",
    "\n",
    "vec_file_sent_ids_dict = {}\n",
    "sent_ids_vec_file_dict = {}\n",
    "\n",
    "for pq_vec_file in pq_vec_files:\n",
    "    print(pq_vec_file)\n",
    "    pq_vec_df = pd.read_parquet(pq_vec_file)\n",
    "    \n",
    "    vec_sent_ids = list(set(pq_vec_df.sentence_id.tolist()))\n",
    "    \n",
    "    for vec_sent_id in vec_sent_ids:\n",
    "        vec_file_sent_ids_dict.setdefault(pq_vec_file, set([])).add(vec_sent_id)\n",
    "        sent_ids_vec_file_dict.setdefault(vec_sent_id, set([])).add(pq_vec_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText3_trunc.csv\n",
      "4408\n",
      "4408\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText1_trunc.csv\n",
      "66\n",
      "66\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText11_trunc.csv\n",
      "178895\n",
      "178895\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText16_trunc.csv\n",
      "2709\n",
      "2709\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText9_trunc.csv\n",
      "369\n",
      "369\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText14_trunc.csv\n",
      "1808\n",
      "1808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText17_trunc.csv\n",
      "2508\n",
      "2508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText13_trunc.csv\n",
      "845\n",
      "845\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText6_trunc.csv\n",
      "539\n",
      "539\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText2_trunc.csv\n",
      "922\n",
      "922\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText7_trunc.csv\n",
      "2721\n",
      "2721\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText18_trunc.csv\n",
      "2508\n",
      "2508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText5_trunc.csv\n",
      "1306\n",
      "1306\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText4_trunc.csv\n",
      "2753\n",
      "2753\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText8_trunc.csv\n",
      "3075\n",
      "3075\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText10_trunc.csv\n",
      "20455\n",
      "20455\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText19_trunc.csv\n",
      "34003\n",
      "34003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText0_trunc.csv\n",
      "6240\n",
      "6240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText12_trunc.csv\n",
      "2257\n",
      "2257\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_truncated_text_files/v8processedText15_trunc.csv\n",
      "2879\n",
      "2879\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, cord_uid, sentence_id, section, sentence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check uniqueness of sentence ids\n",
    "csv_text_files = glob.glob('%s/*' % csv_trunc_text_file_dir)\n",
    "\n",
    "for csv_text_file in csv_text_files:\n",
    "    \n",
    "    text_df = pd.read_csv(csv_text_file)\n",
    "    \n",
    "    print(csv_text_file)\n",
    "    print(len(text_df.sentence_id.tolist()))\n",
    "    print(len(set(text_df.sentence_id.tolist())))\n",
    "\n",
    "    mask = text_df.sentence_id.duplicated(keep=False)\n",
    "    display(text_df[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_preprocessed/v8processedText1.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>subsection</th>\n",
       "      <th>sentence</th>\n",
       "      <th>lemma</th>\n",
       "      <th>UMLS</th>\n",
       "      <th>translated</th>\n",
       "      <th>GGP</th>\n",
       "      <th>...</th>\n",
       "      <th>GENE_OR_GENE_PRODUCT</th>\n",
       "      <th>SIMPLE_CHEMICAL</th>\n",
       "      <th>ANATOMICAL_SYSTEM</th>\n",
       "      <th>IMMATERIAL_ANATOMICAL_ENTITY</th>\n",
       "      <th>MULTI-TISSUE_STRUCTURE</th>\n",
       "      <th>DEVELOPING_ANATOMICAL_STRUCTURE</th>\n",
       "      <th>ORGANISM_SUBDIVISION</th>\n",
       "      <th>CELLULAR_COMPONENT</th>\n",
       "      <th>PATHOLOGICAL_FORMATION</th>\n",
       "      <th>ORGANISM_SUBSTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>thobsldp</td>\n",
       "      <td>en</td>\n",
       "      <td>thobsldp4199</td>\n",
       "      <td>Competitive binding between host immune compon...</td>\n",
       "      <td>41</td>\n",
       "      <td>The statistical significance was determined fr...</td>\n",
       "      <td>[statistical, significance, determine, Student...</td>\n",
       "      <td>[Statistical Significance, Package Dosing Unit]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[â€™s]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>thobsldp</td>\n",
       "      <td>en</td>\n",
       "      <td>thobsldp4199</td>\n",
       "      <td>Confirmation of the immunoglobulin-binding beh...</td>\n",
       "      <td>4</td>\n",
       "      <td>Their interaction with Ig was analyzed using a...</td>\n",
       "      <td>[confirm, Ig-binding, behavior, protein, ident...</td>\n",
       "      <td>[Drug Interactions, Intramuscular immunoglobul...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Ig]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cord_uid language   sentence_id  \\\n",
       "99   thobsldp       en  thobsldp4199   \n",
       "199  thobsldp       en  thobsldp4199   \n",
       "\n",
       "                                               section  subsection  \\\n",
       "99   Competitive binding between host immune compon...          41   \n",
       "199  Confirmation of the immunoglobulin-binding beh...           4   \n",
       "\n",
       "                                              sentence  \\\n",
       "99   The statistical significance was determined fr...   \n",
       "199  Their interaction with Ig was analyzed using a...   \n",
       "\n",
       "                                                 lemma  \\\n",
       "99   [statistical, significance, determine, Student...   \n",
       "199  [confirm, Ig-binding, behavior, protein, ident...   \n",
       "\n",
       "                                                  UMLS  translated GGP  ...  \\\n",
       "99     [Statistical Significance, Package Dosing Unit]       False  []  ...   \n",
       "199  [Drug Interactions, Intramuscular immunoglobul...       False  []  ...   \n",
       "\n",
       "    GENE_OR_GENE_PRODUCT SIMPLE_CHEMICAL ANATOMICAL_SYSTEM  \\\n",
       "99                  [â€™s]              []                []   \n",
       "199                 [Ig]              []                []   \n",
       "\n",
       "    IMMATERIAL_ANATOMICAL_ENTITY MULTI-TISSUE_STRUCTURE  \\\n",
       "99                            []                     []   \n",
       "199                           []                     []   \n",
       "\n",
       "    DEVELOPING_ANATOMICAL_STRUCTURE ORGANISM_SUBDIVISION CELLULAR_COMPONENT  \\\n",
       "99                               []                   []                 []   \n",
       "199                              []                   []                 []   \n",
       "\n",
       "    PATHOLOGICAL_FORMATION ORGANISM_SUBSTANCE  \n",
       "99                      []                 []  \n",
       "199                     []                 []  \n",
       "\n",
       "[2 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_preprocessed/v8processedText4.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>subsection</th>\n",
       "      <th>sentence</th>\n",
       "      <th>lemma</th>\n",
       "      <th>UMLS</th>\n",
       "      <th>translated</th>\n",
       "      <th>GGP</th>\n",
       "      <th>...</th>\n",
       "      <th>GENE_OR_GENE_PRODUCT</th>\n",
       "      <th>SIMPLE_CHEMICAL</th>\n",
       "      <th>ANATOMICAL_SYSTEM</th>\n",
       "      <th>IMMATERIAL_ANATOMICAL_ENTITY</th>\n",
       "      <th>MULTI-TISSUE_STRUCTURE</th>\n",
       "      <th>DEVELOPING_ANATOMICAL_STRUCTURE</th>\n",
       "      <th>ORGANISM_SUBDIVISION</th>\n",
       "      <th>CELLULAR_COMPONENT</th>\n",
       "      <th>PATHOLOGICAL_FORMATION</th>\n",
       "      <th>ORGANISM_SUBSTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>a3k56ulv</td>\n",
       "      <td>en</td>\n",
       "      <td>a3k56ulv2120</td>\n",
       "      <td>Purification and cleavage of SUMO-FGF23 ::: Re...</td>\n",
       "      <td>21</td>\n",
       "      <td>According to the isoelectric point of fusion p...</td>\n",
       "      <td>[According, isoelectric, point, fusion, protei...</td>\n",
       "      <td>[Isoelectric Point, Fusion protein, 2-diethyla...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[SUMO-FGF23]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>a3k56ulv</td>\n",
       "      <td>en</td>\n",
       "      <td>a3k56ulv2120</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>2</td>\n",
       "      <td>Thus, we also cloned a SUMO fragment and const...</td>\n",
       "      <td>[recent, year, small, ubiquitin-related, modif...</td>\n",
       "      <td>[Clone Cells, SUMO-1 Protein, Fragment, Gene E...</td>\n",
       "      <td>False</td>\n",
       "      <td>[SUMO fragment, SUMO]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3djfwd0y</td>\n",
       "      <td>en</td>\n",
       "      <td>3djfwd0y1251</td>\n",
       "      <td>History of Sewage Disposal in Antarctica ::: S...</td>\n",
       "      <td>12</td>\n",
       "      <td>More recently, increasing station populations ...</td>\n",
       "      <td>[recently, increase, station, population, comm...</td>\n",
       "      <td>[Increasing, geographic population, Increase, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Reed]</td>\n",
       "      <td>[piped, Arcone]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3djfwd0y</td>\n",
       "      <td>en</td>\n",
       "      <td>3djfwd0y1252</td>\n",
       "      <td>History of Sewage Disposal in Antarctica ::: S...</td>\n",
       "      <td>12</td>\n",
       "      <td>At coastal bases these were, and largely remai...</td>\n",
       "      <td>[recently, increase, station, population, comm...</td>\n",
       "      <td>[Surface, Body Fluid Discharge, Ascend (action...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[Reed]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Bleasel]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>3djfwd0y</td>\n",
       "      <td>en</td>\n",
       "      <td>3djfwd0y1251</td>\n",
       "      <td>Sewage and Wastewater ::: Sewage Disposal from...</td>\n",
       "      <td>1</td>\n",
       "      <td>For the purposes of this discussion, the term ...</td>\n",
       "      <td>[purpose, discussion, term, sewage, refer, hum...</td>\n",
       "      <td>[Purpose, Discussion (procedure), Sewage, Homo...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>3djfwd0y</td>\n",
       "      <td>en</td>\n",
       "      <td>3djfwd0y1252</td>\n",
       "      <td>Sewage and Wastewater ::: Sewage Disposal from...</td>\n",
       "      <td>1</td>\n",
       "      <td>Elsewhere in the world, sewage treatment plant...</td>\n",
       "      <td>[purpose, discussion, term, sewage, refer, hum...</td>\n",
       "      <td>[World, Sewage, Treatment Plan, Wastewater, In...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[EPA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[surface]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cord_uid language   sentence_id  \\\n",
       "20   a3k56ulv       en  a3k56ulv2120   \n",
       "120  a3k56ulv       en  a3k56ulv2120   \n",
       "51   3djfwd0y       en  3djfwd0y1251   \n",
       "52   3djfwd0y       en  3djfwd0y1252   \n",
       "251  3djfwd0y       en  3djfwd0y1251   \n",
       "252  3djfwd0y       en  3djfwd0y1252   \n",
       "\n",
       "                                               section  subsection  \\\n",
       "20   Purification and cleavage of SUMO-FGF23 ::: Re...          21   \n",
       "120                                       Introduction           2   \n",
       "51   History of Sewage Disposal in Antarctica ::: S...          12   \n",
       "52   History of Sewage Disposal in Antarctica ::: S...          12   \n",
       "251  Sewage and Wastewater ::: Sewage Disposal from...           1   \n",
       "252  Sewage and Wastewater ::: Sewage Disposal from...           1   \n",
       "\n",
       "                                              sentence  \\\n",
       "20   According to the isoelectric point of fusion p...   \n",
       "120  Thus, we also cloned a SUMO fragment and const...   \n",
       "51   More recently, increasing station populations ...   \n",
       "52   At coastal bases these were, and largely remai...   \n",
       "251  For the purposes of this discussion, the term ...   \n",
       "252  Elsewhere in the world, sewage treatment plant...   \n",
       "\n",
       "                                                 lemma  \\\n",
       "20   [According, isoelectric, point, fusion, protei...   \n",
       "120  [recent, year, small, ubiquitin-related, modif...   \n",
       "51   [recently, increase, station, population, comm...   \n",
       "52   [recently, increase, station, population, comm...   \n",
       "251  [purpose, discussion, term, sewage, refer, hum...   \n",
       "252  [purpose, discussion, term, sewage, refer, hum...   \n",
       "\n",
       "                                                  UMLS  translated  \\\n",
       "20   [Isoelectric Point, Fusion protein, 2-diethyla...       False   \n",
       "120  [Clone Cells, SUMO-1 Protein, Fragment, Gene E...       False   \n",
       "51   [Increasing, geographic population, Increase, ...       False   \n",
       "52   [Surface, Body Fluid Discharge, Ascend (action...       False   \n",
       "251  [Purpose, Discussion (procedure), Sewage, Homo...       False   \n",
       "252  [World, Sewage, Treatment Plan, Wastewater, In...       False   \n",
       "\n",
       "                       GGP  ... GENE_OR_GENE_PRODUCT  SIMPLE_CHEMICAL  \\\n",
       "20                      []  ...         [SUMO-FGF23]               []   \n",
       "120  [SUMO fragment, SUMO]  ...                   []               []   \n",
       "51                      []  ...               [Reed]  [piped, Arcone]   \n",
       "52                      []  ...               [Reed]               []   \n",
       "251                     []  ...                   []               []   \n",
       "252                     []  ...                   []            [EPA]   \n",
       "\n",
       "    ANATOMICAL_SYSTEM IMMATERIAL_ANATOMICAL_ENTITY MULTI-TISSUE_STRUCTURE  \\\n",
       "20                 []                           []                     []   \n",
       "120                []                           []                     []   \n",
       "51                 []                           []                     []   \n",
       "52                 []                    [Bleasel]                     []   \n",
       "251                []                           []                     []   \n",
       "252                []                           []                     []   \n",
       "\n",
       "    DEVELOPING_ANATOMICAL_STRUCTURE ORGANISM_SUBDIVISION CELLULAR_COMPONENT  \\\n",
       "20                               []                   []                 []   \n",
       "120                              []                   []                 []   \n",
       "51                               []                   []                 []   \n",
       "52                               []                   []                 []   \n",
       "251                              []                   []                 []   \n",
       "252                              []                   []          [surface]   \n",
       "\n",
       "    PATHOLOGICAL_FORMATION ORGANISM_SUBSTANCE  \n",
       "20                      []                 []  \n",
       "120                     []                 []  \n",
       "51                      []                 []  \n",
       "52                      []                 []  \n",
       "251                     []                 []  \n",
       "252                     []                 []  \n",
       "\n",
       "[6 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_preprocessed/v8processedText8.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>subsection</th>\n",
       "      <th>sentence</th>\n",
       "      <th>lemma</th>\n",
       "      <th>UMLS</th>\n",
       "      <th>translated</th>\n",
       "      <th>GGP</th>\n",
       "      <th>...</th>\n",
       "      <th>GENE_OR_GENE_PRODUCT</th>\n",
       "      <th>SIMPLE_CHEMICAL</th>\n",
       "      <th>ANATOMICAL_SYSTEM</th>\n",
       "      <th>IMMATERIAL_ANATOMICAL_ENTITY</th>\n",
       "      <th>MULTI-TISSUE_STRUCTURE</th>\n",
       "      <th>DEVELOPING_ANATOMICAL_STRUCTURE</th>\n",
       "      <th>ORGANISM_SUBDIVISION</th>\n",
       "      <th>CELLULAR_COMPONENT</th>\n",
       "      <th>PATHOLOGICAL_FORMATION</th>\n",
       "      <th>ORGANISM_SUBSTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cord_uid, language, sentence_id, section, subsection, sentence, lemma, UMLS, translated, GGP, SO, TAXON, CHEBI, GO, CL, DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN, DISEASE, CHEMICAL, CANCER, ORGAN, TISSUE, ORGANISM, CELL, AMINO_ACID, GENE_OR_GENE_PRODUCT, SIMPLE_CHEMICAL, ANATOMICAL_SYSTEM, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, DEVELOPING_ANATOMICAL_STRUCTURE, ORGANISM_SUBDIVISION, CELLULAR_COMPONENT, PATHOLOGICAL_FORMATION, ORGANISM_SUBSTANCE]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_preprocessed/v8processedText10.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>section</th>\n",
       "      <th>subsection</th>\n",
       "      <th>sentence</th>\n",
       "      <th>lemma</th>\n",
       "      <th>UMLS</th>\n",
       "      <th>translated</th>\n",
       "      <th>GGP</th>\n",
       "      <th>...</th>\n",
       "      <th>GENE_OR_GENE_PRODUCT</th>\n",
       "      <th>SIMPLE_CHEMICAL</th>\n",
       "      <th>ANATOMICAL_SYSTEM</th>\n",
       "      <th>IMMATERIAL_ANATOMICAL_ENTITY</th>\n",
       "      <th>MULTI-TISSUE_STRUCTURE</th>\n",
       "      <th>DEVELOPING_ANATOMICAL_STRUCTURE</th>\n",
       "      <th>ORGANISM_SUBDIVISION</th>\n",
       "      <th>CELLULAR_COMPONENT</th>\n",
       "      <th>PATHOLOGICAL_FORMATION</th>\n",
       "      <th>ORGANISM_SUBSTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>hdf2mwpc</td>\n",
       "      <td>en</td>\n",
       "      <td>hdf2mwpc180</td>\n",
       "      <td>Foal Diarrhea 679</td>\n",
       "      <td>18</td>\n",
       "      <td>The diagnosis of Salmonella infection is tradi...</td>\n",
       "      <td>[diagnosis, Salmonella, infection, traditional...</td>\n",
       "      <td>[Diagnosis Study, Salmonella infections, Stool...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hdf2mwpc</td>\n",
       "      <td>en</td>\n",
       "      <td>hdf2mwpc181</td>\n",
       "      <td>Foal Diarrhea 679</td>\n",
       "      <td>18</td>\n",
       "      <td>Samples should be transported using suitable t...</td>\n",
       "      <td>[diagnosis, Salmonella, infection, traditional...</td>\n",
       "      <td>[Specimen, Membrane Transport Proteins, Transp...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>hdf2mwpc</td>\n",
       "      <td>en</td>\n",
       "      <td>hdf2mwpc182</td>\n",
       "      <td>Foal Diarrhea 679</td>\n",
       "      <td>18</td>\n",
       "      <td>Samples can be transported in selenite broth i...</td>\n",
       "      <td>[diagnosis, Salmonella, infection, traditional...</td>\n",
       "      <td>[Specimen, Membrane Transport Proteins, Collec...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[selenite]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>hdf2mwpc</td>\n",
       "      <td>en</td>\n",
       "      <td>hdf2mwpc183</td>\n",
       "      <td>Foal Diarrhea 679</td>\n",
       "      <td>18</td>\n",
       "      <td>Blood culture is worthwhile in foals less than...</td>\n",
       "      <td>[diagnosis, Salmonella, infection, traditional...</td>\n",
       "      <td>[Blood culture, month, Age]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>hdf2mwpc</td>\n",
       "      <td>en</td>\n",
       "      <td>hdf2mwpc180</td>\n",
       "      <td>FOAL HEAT DIARRHEA</td>\n",
       "      <td>1</td>\n",
       "      <td>The basis of foal heat diarrhea is not certain...</td>\n",
       "      <td>[Foal, heat, diarrhea, arguably, common, cause...</td>\n",
       "      <td>[Horse under one year old, Biologic Developmen...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>hdf2mwpc</td>\n",
       "      <td>en</td>\n",
       "      <td>hdf2mwpc181</td>\n",
       "      <td>FOAL HEAT DIARRHEA</td>\n",
       "      <td>1</td>\n",
       "      <td>The temporal association between coprophagy, t...</td>\n",
       "      <td>[Foal, heat, diarrhea, arguably, common, cause...</td>\n",
       "      <td>[Temporal - Regional site descriptor, Relation...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[intestinal flora]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>hdf2mwpc</td>\n",
       "      <td>en</td>\n",
       "      <td>hdf2mwpc182</td>\n",
       "      <td>FOAL HEAT DIARRHEA</td>\n",
       "      <td>1</td>\n",
       "      <td>The diarrhea does not appear to be associated ...</td>\n",
       "      <td>[Foal, heat, diarrhea, arguably, common, cause...</td>\n",
       "      <td>[Diarrhea, Associated with, Alteration, Compos...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>hdf2mwpc</td>\n",
       "      <td>en</td>\n",
       "      <td>hdf2mwpc183</td>\n",
       "      <td>FOAL HEAT DIARRHEA</td>\n",
       "      <td>1</td>\n",
       "      <td>Anecdotal reports suggest that the feeding of ...</td>\n",
       "      <td>[Foal, heat, diarrhea, arguably, common, cause...</td>\n",
       "      <td>[Anecdotal Report, Feeding patient, Biotin, In...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cord_uid language  sentence_id             section  subsection  \\\n",
       "0   hdf2mwpc       en  hdf2mwpc180   Foal Diarrhea 679          18   \n",
       "1   hdf2mwpc       en  hdf2mwpc181   Foal Diarrhea 679          18   \n",
       "2   hdf2mwpc       en  hdf2mwpc182   Foal Diarrhea 679          18   \n",
       "3   hdf2mwpc       en  hdf2mwpc183   Foal Diarrhea 679          18   \n",
       "80  hdf2mwpc       en  hdf2mwpc180  FOAL HEAT DIARRHEA           1   \n",
       "81  hdf2mwpc       en  hdf2mwpc181  FOAL HEAT DIARRHEA           1   \n",
       "82  hdf2mwpc       en  hdf2mwpc182  FOAL HEAT DIARRHEA           1   \n",
       "83  hdf2mwpc       en  hdf2mwpc183  FOAL HEAT DIARRHEA           1   \n",
       "\n",
       "                                             sentence  \\\n",
       "0   The diagnosis of Salmonella infection is tradi...   \n",
       "1   Samples should be transported using suitable t...   \n",
       "2   Samples can be transported in selenite broth i...   \n",
       "3   Blood culture is worthwhile in foals less than...   \n",
       "80  The basis of foal heat diarrhea is not certain...   \n",
       "81  The temporal association between coprophagy, t...   \n",
       "82  The diarrhea does not appear to be associated ...   \n",
       "83  Anecdotal reports suggest that the feeding of ...   \n",
       "\n",
       "                                                lemma  \\\n",
       "0   [diagnosis, Salmonella, infection, traditional...   \n",
       "1   [diagnosis, Salmonella, infection, traditional...   \n",
       "2   [diagnosis, Salmonella, infection, traditional...   \n",
       "3   [diagnosis, Salmonella, infection, traditional...   \n",
       "80  [Foal, heat, diarrhea, arguably, common, cause...   \n",
       "81  [Foal, heat, diarrhea, arguably, common, cause...   \n",
       "82  [Foal, heat, diarrhea, arguably, common, cause...   \n",
       "83  [Foal, heat, diarrhea, arguably, common, cause...   \n",
       "\n",
       "                                                 UMLS  translated GGP  ...  \\\n",
       "0   [Diagnosis Study, Salmonella infections, Stool...       False  []  ...   \n",
       "1   [Specimen, Membrane Transport Proteins, Transp...       False  []  ...   \n",
       "2   [Specimen, Membrane Transport Proteins, Collec...       False  []  ...   \n",
       "3                         [Blood culture, month, Age]       False  []  ...   \n",
       "80  [Horse under one year old, Biologic Developmen...       False  []  ...   \n",
       "81  [Temporal - Regional site descriptor, Relation...       False  []  ...   \n",
       "82  [Diarrhea, Associated with, Alteration, Compos...       False  []  ...   \n",
       "83  [Anecdotal Report, Feeding patient, Biotin, In...       False  []  ...   \n",
       "\n",
       "   GENE_OR_GENE_PRODUCT SIMPLE_CHEMICAL ANATOMICAL_SYSTEM  \\\n",
       "0                    []              []                []   \n",
       "1                    []              []                []   \n",
       "2                    []      [selenite]                []   \n",
       "3                    []              []                []   \n",
       "80                   []              []                []   \n",
       "81                   []              []                []   \n",
       "82                   []              []                []   \n",
       "83                   []              []                []   \n",
       "\n",
       "   IMMATERIAL_ANATOMICAL_ENTITY MULTI-TISSUE_STRUCTURE  \\\n",
       "0                            []                     []   \n",
       "1                            []                     []   \n",
       "2                            []                     []   \n",
       "3                            []                     []   \n",
       "80                           []                     []   \n",
       "81                           []                     []   \n",
       "82                           []                     []   \n",
       "83                           []                     []   \n",
       "\n",
       "   DEVELOPING_ANATOMICAL_STRUCTURE ORGANISM_SUBDIVISION CELLULAR_COMPONENT  \\\n",
       "0                               []                   []                 []   \n",
       "1                               []                   []                 []   \n",
       "2                               []                   []                 []   \n",
       "3                               []                   []                 []   \n",
       "80                              []                   []                 []   \n",
       "81                              []                   []                 []   \n",
       "82                              []                   []                 []   \n",
       "83                              []                   []                 []   \n",
       "\n",
       "   PATHOLOGICAL_FORMATION ORGANISM_SUBSTANCE  \n",
       "0                      []                 []  \n",
       "1                      []                 []  \n",
       "2                      []                 []  \n",
       "3                      []                 []  \n",
       "80                     []                 []  \n",
       "81     [intestinal flora]                 []  \n",
       "82                     []                 []  \n",
       "83                     []                 []  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/v8_preprocessed/v8processedText3.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2bd6ae07fab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpq_text_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpq_text_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpq_text_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtext_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpq_text_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_pandas_metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         result = self.api.parquet.read_table(\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         ).to_pandas()\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size)\u001b[0m\n\u001b[1;32m   1279\u001b[0m                          buffer_size=buffer_size)\n\u001b[1;32m   1280\u001b[0m     return pf.read(columns=columns, use_threads=use_threads,\n\u001b[0;32m-> 1281\u001b[0;31m                    use_pandas_metadata=use_pandas_metadata)\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m             table = piece.read(columns=columns, use_threads=use_threads,\n\u001b[1;32m   1136\u001b[0m                                \u001b[0mpartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m                                use_pandas_metadata=use_pandas_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m             \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, columns, use_threads, partitions, file, use_pandas_metadata)\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_row_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m    251\u001b[0m             columns, use_pandas_metadata=use_pandas_metadata)\n\u001b[1;32m    252\u001b[0m         return self.reader.read_all(column_indices=column_indices,\n\u001b[0;32m--> 253\u001b[0;31m                                     use_threads=use_threads)\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscan_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pq_text_files = glob.glob('%s/*' % pq_text_file_dir)\n",
    "\n",
    "for pq_text_file in pq_text_files:\n",
    "    print(pq_text_file)\n",
    "    text_df = pd.read_parquet(pq_text_file)\n",
    "    \n",
    "    mask = text_df.sentence_id.duplicated(keep=False)\n",
    "    display(text_df[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 12-13: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6ecabae45b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcsv_vec_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_vec_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtext_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_vec_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(csv_vec_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 12-13: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "csv_vec_files = glob.glob('%s/*' % csv_trunc_vec_file_dir)\n",
    "\n",
    "for csv_vec_file in csv_vec_files:\n",
    "    \n",
    "    text_df = pd.read_csv(csv_vec_file)\n",
    "\n",
    "    print(csv_vec_file)\n",
    "    \n",
    "    mask = text_df.sentence_id.duplicated(keep=False)\n",
    "    display(text_df[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328552\n",
      "328552\n",
      "330021\n",
      "328552\n",
      "328552\n",
      "330021\n",
      "resources/v8_vectors/v8processedVecs4.parquet\n",
      "330021\n",
      "330021\n",
      "328553\n",
      "328553\n",
      "330020\n",
      "328553\n",
      "330020\n",
      "330021\n",
      "328553\n",
      "328553\n",
      "328553\n",
      "330021\n",
      "330020\n",
      "330020\n"
     ]
    }
   ],
   "source": [
    "pq_vec_files = glob.glob('%s/*' % pq_vec_file_dir)\n",
    "\n",
    "for pq_vec_file in pq_vec_files:\n",
    "    \n",
    "    text_df = pd.read_parquet(pq_vec_file)\n",
    "\n",
    "    sentence_ids = text_df.sentence_id.tolist()\n",
    "    print(len(sentence_ids))\n",
    "    if \"thobsldp4199\" in text_df.sentence_id.tolist():\n",
    "        print(pq_vec_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = pd.read_parquet('resources/v8_vectors/v8processedVecs4.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare prior and current dataframe\n",
    "\n",
    "prior_df = pd.read_csv('resources/filtered_text_vec_df_200429.csv')\n",
    "current_df = pd.read_csv('resources/filt_merged_text_vector_df_200430.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4677\n",
      "3393\n"
     ]
    }
   ],
   "source": [
    "print(len(set(current_df.cord_uid.tolist())))\n",
    "print(len(set(prior_df.cord_uid.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
